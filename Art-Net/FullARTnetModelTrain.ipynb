{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brief-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import cv2\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend as keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "import tensorflow as tf \n",
    "\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import glob\n",
    "import itertools\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage.morphology import disk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#from sklearn.metrics import jaccard_similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-model",
   "metadata": {},
   "source": [
    "## cpu configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "transparent-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-wealth",
   "metadata": {},
   "source": [
    "## the proposed network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conservative-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The implementation details with the explanation can be found on the paper. \n",
    "Please, read the paper first then see the source code.\n",
    "'''\n",
    "def SIMOCNN(nClasses, input_height, input_width):\n",
    "    \n",
    "    '''\n",
    "    Load VGG16 from keras and initialize with the ImageNet. The output of the VGG16\n",
    "    which is pool5 is used as input to all the sub-networks. This part of the network\n",
    "    is responsible for the feature extraction which is so called convolution part \n",
    "    of the semantic segmentation CNN. \n",
    "    '''\n",
    "    \n",
    "    # defining the Input shape where channel 3 means RGB.  \n",
    "    img_input = Input(shape=(input_height, input_width, 3)) \n",
    "\n",
    "    vgg_Base = VGG16(weights = 'imagenet',\n",
    "                     include_top = False,\n",
    "                     input_tensor = img_input) \n",
    "    \n",
    "    '''\n",
    "    To overcome the sub-sampling limitations  and  deconvolution  overlap, we  have\n",
    "    employed two types of skip connections. First one is between the corresponding\n",
    "    same dimensional feature map in both encoder and decoder which  has  ladder like\n",
    "    structure and  it  is  inspired  from  U-Net. Second  one, so called FrG connect\n",
    "    the very end layer of the decoder with the original image via stack of depthwise\n",
    "    separable  convolution  without sub-sampling  to  produce  the  fully resolution\n",
    "    feature map. Second skip connection is the compensatory of losing spacial \n",
    "    information dueto sub-sampling by concatenating the full resolution feature map.  \n",
    "    In the presented SIMO-DCNN network, both  the  segmentation  and  regression  \n",
    "    sub-networks  follow  our proposedencoder-decoder  networks  to  get  full  \n",
    "    resolution  features  map.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    FrG = SeparableConv2D(filters = 64,\n",
    "                          kernel_size = (3, 3), \n",
    "                          activation = 'relu',\n",
    "                          kernel_initializer='glorot_uniform',\n",
    "                          padding=\"same\")(img_input)\n",
    "    FrG = BatchNormalization()(FrG)\n",
    "    \n",
    "    FrG = SeparableConv2D(filters = 256,\n",
    "                          kernel_size = (3, 3),\n",
    "                          activation = 'relu', \n",
    "                          kernel_initializer='glorot_uniform', \n",
    "                          padding=\"same\")(FrG)\n",
    "    FrG = BatchNormalization()(FrG)\n",
    "        \n",
    "    FrG = SeparableConv2D(filters = 64,\n",
    "                          kernel_size = (3, 3), \n",
    "                          activation = 'relu',\n",
    "                          kernel_initializer='glorot_uniform',\n",
    "                          padding=\"same\")(FrG)\n",
    "    FrG = BatchNormalization()(FrG)\n",
    "    \n",
    "    FrG = SeparableConv2D(filters = nClasses,\n",
    "                          kernel_size = (3, 3), \n",
    "                          activation = 'relu',\n",
    "                          kernel_initializer='glorot_uniform',\n",
    "                          padding=\"same\")(FrG)\n",
    "    FrG = BatchNormalization()(FrG)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Decoding the encoded output to semantically project the discriminating features \n",
    "    of  lower  resolution learnt  by  the  encoder  onto  the  pixel space  of \n",
    "    higher  resolution  to  get a dense pixel wise classification.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    conv_14 = SeparableConv2D(filters = 1024, \n",
    "                            kernel_size = (3, 3), \n",
    "                            activation = 'relu', \n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            padding=\"same\")(vgg_Base.output)\n",
    "    conv_14 = BatchNormalization()(conv_14)\n",
    "\n",
    "\n",
    "    conv_15 = SeparableConv2D(filters = 1024, \n",
    "                              kernel_size = (3, 3), \n",
    "                              activation = 'relu', \n",
    "                              kernel_initializer='glorot_uniform', \n",
    "                              padding=\"same\")(conv_14)\n",
    "    conv_15 = BatchNormalization()(conv_15)\n",
    "\n",
    "    \n",
    "    deconv_1 = UpSampling2D(size = (2, 2))(conv_15)\n",
    "    deconv_1 = concatenate([vgg_Base.get_layer(name=\"block4_pool\").output,\n",
    "                            deconv_1], axis=-1)\n",
    "    deconv_1 = SeparableConv2D(filters = 512, \n",
    "                               kernel_size = (3, 3), \n",
    "                               activation = 'relu', \n",
    "                               kernel_initializer='glorot_uniform', \n",
    "                               padding = \"same\")(deconv_1)\n",
    "    deconv_1 = BatchNormalization()(deconv_1)\n",
    "\n",
    "\n",
    "    deconv_2 = UpSampling2D(size = (2, 2))(deconv_1)\n",
    "    deconv_2 = concatenate([vgg_Base.get_layer(name=\"block3_pool\").output,\n",
    "                            deconv_2], axis=-1)\n",
    "    deconv_2 = SeparableConv2D(filters = 256,\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               padding = \"same\")(deconv_2)\n",
    "    deconv_2 = BatchNormalization()(deconv_2)\n",
    "\n",
    "\n",
    "    deconv_3 = UpSampling2D( size = (2, 2))(deconv_2)\n",
    "    deconv_3 = concatenate([vgg_Base.get_layer(name=\"block2_pool\").output,\n",
    "                            deconv_3], axis=-1)\n",
    "    deconv_3 = SeparableConv2D(filters = 128,\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               padding = \"same\")(deconv_3)\n",
    "    kept = BatchNormalization()(deconv_3)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    '''\n",
    "    DBRS blocks named as segmentation sub-network for semantic tissue or\n",
    "    instrument pixels labelling to get semantic segmentation of the surgical tool.\n",
    "    '''\n",
    "    \n",
    "    tool = UpSampling2D(size = (2, 2))(kept)\n",
    "    tool = concatenate([vgg_Base.get_layer( name=\"block1_pool\").output, \n",
    "                        tool], axis=-1)\n",
    "    \n",
    "    tool = SeparableConv2D(filters = 64,\n",
    "                           kernel_size = (3, 3), \n",
    "                           activation = 'relu',\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           padding = \"same\")(tool)\n",
    "    tool = BatchNormalization()(tool)\n",
    "\n",
    "    tool = UpSampling2D(size = (2, 2))(tool)\n",
    "    tool = SeparableConv2D(filters = 64, \n",
    "                           kernel_size = (3, 3), \n",
    "                           activation = 'relu',\n",
    "                           kernel_initializer='glorot_uniform', \n",
    "                           padding = \"same\")(tool)\n",
    "    tool = BatchNormalization()(tool)\n",
    "\n",
    "    tool = SeparableConv2D(filters = nClasses,\n",
    "                           kernel_size = (1, 1),\n",
    "                           activation = 'relu',\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           padding = \"same\")(tool)\n",
    "    tool = BatchNormalization()(tool)\n",
    "\n",
    "    tool = concatenate([tool, FrG], axis=-1)\n",
    "\n",
    "    tool = Conv2D(filters = 1,\n",
    "                  kernel_size = 1,\n",
    "                  activation = 'sigmoid',\n",
    "                  name='tool')(tool)\n",
    " \n",
    "    modeltool = Model(inputs = img_input, outputs = tool)    \n",
    "    modeltool.load_weights('/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/Models/pretrainedRobotic_editedFinal2.h5')\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    DBR blocks named as regression sub-network for mid-line feature of the surgical\n",
    "    tool for pose estimation. \n",
    "    '''\n",
    "\n",
    "    midline = UpSampling2D(size = (2, 2))(kept)\n",
    "    midline = concatenate([vgg_Base.get_layer( name=\"block1_pool\").output, \n",
    "                           midline], axis=-1)\n",
    "    \n",
    "    midline = SeparableConv2D(filters = 64,\n",
    "                              kernel_size = (3, 3),\n",
    "                              activation = 'relu',\n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              padding = \"same\")(midline)\n",
    "    midline = BatchNormalization()(midline)\n",
    "\n",
    "    midline = UpSampling2D(size = (2, 2))(midline)\n",
    "    midline = SeparableConv2D(filters = 64, \n",
    "                              kernel_size = (3, 3),activation = 'relu',\n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              padding = \"same\")(midline)\n",
    "    midline = BatchNormalization()(midline)\n",
    "\n",
    "    midline = SeparableConv2D(filters = nClasses, \n",
    "                              kernel_size = (1, 1),\n",
    "                              activation = 'relu',\n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              padding = \"same\")(midline)\n",
    "    midline = BatchNormalization()(midline)\n",
    "\n",
    "    midline = concatenate([midline, FrG], axis=-1)\n",
    "\n",
    "    midline = Conv2D(filters = 1,\n",
    "                     kernel_size = 1,\n",
    "                     activation = 'sigmoid',\n",
    "                     name='midline')(midline)\n",
    " \n",
    "    modelmidline = Model(inputs = img_input, outputs = midline)    \n",
    "    modelmidline.load_weights('/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/Models/pretrainedRobotic_editedFinal2.h5')\n",
    " \n",
    "\n",
    "\n",
    "    '''\n",
    "    DBR blocks named as regression sub-network for tool-tip feature of the surgical\n",
    "    tool for pose estimation. \n",
    "    '''\n",
    "\n",
    "    tooltip = UpSampling2D(size = (2, 2))(kept)\n",
    "    tooltip = concatenate([vgg_Base.get_layer( name=\"block1_pool\").output,\n",
    "                           tooltip], axis=-1)\n",
    "    \n",
    "    tooltip = SeparableConv2D(filters = 64,\n",
    "                              kernel_size = (3, 3),\n",
    "                              activation = 'relu',\n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              padding = \"same\")(tooltip)\n",
    "    tooltip = BatchNormalization()(tooltip)\n",
    "\n",
    "    tooltip = UpSampling2D(size = (2, 2))(tooltip)\n",
    "    tooltip = SeparableConv2D(filters = 64,\n",
    "                              kernel_size = (3, 3),\n",
    "                              activation = 'relu',\n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              padding = \"same\")(tooltip)\n",
    "    tooltip = BatchNormalization()(tooltip)\n",
    "\n",
    "    tooltip = SeparableConv2D(filters = nClasses,\n",
    "                              kernel_size = (1, 1),\n",
    "                              activation = 'relu', \n",
    "                              kernel_initializer='glorot_uniform', \n",
    "                              padding = \"same\")(tooltip)\n",
    "    tooltip = BatchNormalization()(tooltip)\n",
    "\n",
    "    tooltip = concatenate([tooltip, FrG], axis=-1)\n",
    "\n",
    "    tooltip = Conv2D(filters = 1,\n",
    "                     kernel_size = 1,\n",
    "                     activation = 'sigmoid',\n",
    "                     name='tooltip')(tooltip)\n",
    " \n",
    "    modeltooltip = Model(inputs = img_input, outputs = tooltip)    \n",
    "    modeltooltip.load_weights('/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/Models/pretrainedRobotic_editedFinal2.h5')\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    DBR blocks named as regression sub-network for Edge-line feature of the surgical\n",
    "    tool for pose estimation. \n",
    "    '''\n",
    "\n",
    "    edgeline = UpSampling2D(size = (2, 2))(kept)\n",
    "    edgeline = concatenate([vgg_Base.get_layer( name=\"block1_pool\").output,\n",
    "                            edgeline], axis=-1)\n",
    "    \n",
    "    edgeline = SeparableConv2D(filters = 64,\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               padding = \"same\")(edgeline)\n",
    "    edgeline = BatchNormalization()(edgeline)\n",
    "\n",
    "    edgeline = UpSampling2D(size = (2, 2))(edgeline)\n",
    "    edgeline = SeparableConv2D(filters = 64,\n",
    "                               kernel_size = (3, 3),\n",
    "                               activation = 'relu',\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               padding = \"same\")(edgeline)\n",
    "    edgeline = BatchNormalization()(edgeline)\n",
    "\n",
    "    edgeline = SeparableConv2D(filters = nClasses,\n",
    "                               kernel_size = (1, 1),\n",
    "                               activation = 'relu',\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               padding = \"same\")(edgeline)\n",
    "    edgeline = BatchNormalization()(edgeline)\n",
    "\n",
    "    edgeline = concatenate([edgeline, FrG], axis=-1)\n",
    "\n",
    "    edgeline = Conv2D(filters = 1,\n",
    "                      kernel_size = 1,\n",
    "                      activation = 'sigmoid',name='edgeline')(edgeline)\n",
    " \n",
    "    modeledgeline = Model(inputs = img_input, outputs = edgeline)    \n",
    "    modeledgeline.load_weights('/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/Models/pretrainedRobotic_editedFinal2.h5')\n",
    "\n",
    "\n",
    "    '''\n",
    "    Detection sub-network for getting the tool flag that will indicate \n",
    "    either pose will estimate or not? \n",
    "    '''\n",
    "    x = modeltool.get_layer('block5_conv3').output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2, activation='softmax',name='detection')(x)\n",
    "    \n",
    "    modeldetection = Model(inputs=img_input, outputs=x)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    SIMO model is build by returning the multiple output as a list variable. \n",
    "    In the output prediction the sequence of the outputs are as follows:\n",
    "    \n",
    "    Output[0]= Predicted Probabilty map fo the surgical tool segmentation.\n",
    "    Output[1]= Predicted Regression map for the mid-line of the surgical tool.\n",
    "    Output[2]= Predicted Regression map for the tool-tip of the surgical tool.\n",
    "    Output[3]= Predicted Regression map for the edge-line of the surgical tool.\n",
    "    Output[4]= Predicted softmax probability of the tool detection.\n",
    "    '''\n",
    "      \n",
    "    SIMO = Model(inputs = img_input, outputs = [modeltool.output,\n",
    "                                              modelmidline.output,\n",
    "                                              modeltooltip.output,\n",
    "                                              modeledgeline.output,\n",
    "                                              modeldetection.output])\n",
    "    \n",
    "#     for layer in SIMO.layers[:40]:\n",
    "#         layer.trainable = False    \n",
    "\n",
    "    return SIMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-internship",
   "metadata": {},
   "source": [
    "## Data Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vertical-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataGenerator(data, batch_size): \n",
    "    '''\n",
    "    This function is for the data generator for fit_generator training of the SIMO\n",
    "    model. \n",
    "    Input Argument: \n",
    "          data = Is the numpy array having 6 column and N row. N is the number of \n",
    "          images as training/ testing sample. It can be pressented as below-\n",
    "          \n",
    "          orgImage | tool mask | edge-line | mid-line | tool-tip | label |\n",
    "          ---------|-----------|-----------|----------|----------|-------|\n",
    "          \n",
    "          ---------|-----------|-----------|----------|----------|-------|\n",
    "          batch_size = the number of samples that will perform forward-backward pass\n",
    "                       in a single shot.\n",
    "    Output Argument:\n",
    "        Tuple of the true images and corresponding mask/ label for each types\n",
    "        of sub-network. \n",
    "    '''\n",
    "    \n",
    "    img = np.array([i[0] for i in data]).reshape(-1,192,256,3)\n",
    "    mask = np.array([i[1] for i in data]).reshape(-1,192,256,1)\n",
    "    edge = np.array([i[2] for i in data]).reshape(-1,192,256,1)\n",
    "    mid  = np.array([i[3] for i in data]).reshape(-1,192,256,1)\n",
    "    tip = np.array([i[4] for i in data]).reshape(-1,192,256,1)\n",
    "    label = np.array([i[5] for i in data])\n",
    "    \n",
    "    label=to_categorical(label, num_classes=2, dtype='float32')\n",
    "\n",
    "    zipped = itertools.cycle( zip(img, mask, mid, edge, tip, label))\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        X = [] \n",
    "        Y = []\n",
    "        Z = []\n",
    "        A = []\n",
    "        C = []   \n",
    "        D = []\n",
    "        for _ in range( batch_size):\n",
    "            im , sg, sg_mid, sg_edge, sg_tip, lab = next(zipped)\n",
    "            X.append(im)\n",
    "            Y.append(sg)\n",
    "            Z.append(sg_mid)\n",
    "            A.append(sg_edge)\n",
    "            C.append(sg_tip)\n",
    "            D.append(lab)\n",
    "            \n",
    "        yield (np.array(X) , {'tool':np.array(Y),\n",
    "                              'midline':np.array(Z),\n",
    "                              'tooltip':np.array(C),\n",
    "                              'edgeline':np.array(A),\n",
    "                              'detection':np.array(D)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-feedback",
   "metadata": {},
   "source": [
    "## Loss Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sublime-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def IoU(y_true, y_pred):\n",
    "        \n",
    "    ''' \n",
    "    The Intersection over Union (IoU) also referred to as the Jaccard index (JI),\n",
    "    is essentially a method to quantify the percent overlap between the GT mask\n",
    "    and prediction output. The IoU metric measures the number of pixels common \n",
    "    between the target and prediction masks divided by the total number of pixels\n",
    "    present across both masks.\n",
    "  \n",
    "    Input Arguments: \n",
    "        y_true: True Labels of the 2D images so called ground truth (GT).\n",
    "        y_pred: Predicted Labels of the 2D images so called Predicted/ segmented Mask.\n",
    "        \n",
    "    Output Arguments: \n",
    "\n",
    "        iou: The IoU between y_true and y_pred\n",
    "\n",
    "    Author: Md. Kamrul Hasan, \n",
    "            Erasmus Scholar on Medical Imaging and Application (MAIA)\n",
    "            E-mail: kamruleeekuet@gmail.com\n",
    "\n",
    "    '''\n",
    "    \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection) / (K.sum(y_true_f) + K.sum(y_pred_f)-intersection)\n",
    "\n",
    "\n",
    "def IoU_loss(y_true, y_pred):\n",
    "    return 1-IoU(y_true, y_pred)\n",
    "\n",
    "\n",
    "def bce_IoU_loss(y_true, y_pred):\n",
    "    return (binary_crossentropy(y_true, y_pred) + IoU_loss(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-journal",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-origin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 6) (199, 6)\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 192, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 192, 256, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 192, 256, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 96, 128, 64)  0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 96, 128, 128) 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 96, 128, 128) 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 48, 64, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 48, 64, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 48, 64, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 48, 64, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 24, 32, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 24, 32, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 24, 32, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 24, 32, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 12, 16, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 12, 16, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 12, 16, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 12, 16, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 6, 8, 512)    0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 6, 8, 1024)   529920      block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 6, 8, 1024)   4096        separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 6, 8, 1024)   1058816     batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 6, 8, 1024)   4096        separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 12, 16, 1024) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 12, 16, 1536) 0           block4_pool[0][0]                \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 12, 16, 512)  800768      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 12, 16, 512)  2048        separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 24, 32, 512)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 24, 32, 768)  0           block3_pool[0][0]                \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 24, 32, 256)  203776      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 24, 32, 256)  1024        separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 48, 64, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 48, 64, 384)  0           block2_pool[0][0]                \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 48, 64, 128)  52736       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 48, 64, 128)  512         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 96, 128, 128) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 96, 128, 128) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 96, 128, 128) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 96, 128, 128) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 96, 128, 192) 0           block1_pool[0][0]                \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 192, 256, 64) 283         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 96, 128, 192) 0           block1_pool[0][0]                \n",
      "                                                                 up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 96, 128, 192) 0           block1_pool[0][0]                \n",
      "                                                                 up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 96, 128, 192) 0           block1_pool[0][0]                \n",
      "                                                                 up_sampling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 96, 128, 64)  14080       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 192, 256, 64) 256         separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 96, 128, 64)  14080       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 96, 128, 64)  14080       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 96, 128, 64)  14080       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 96, 128, 64)  256         separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 192, 256, 256 17216       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 96, 128, 64)  256         separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 96, 128, 64)  256         separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 96, 128, 64)  256         separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 192, 256, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 192, 256, 256 1024        separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 192, 256, 64) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 192, 256, 64) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 192, 256, 64) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 192, 256, 64) 4736        up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 192, 256, 64) 18752       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 192, 256, 64) 4736        up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 192, 256, 64) 4736        up_sampling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 192, 256, 64) 4736        up_sampling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 192, 256, 64) 256         separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 192, 256, 64) 256         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 192, 256, 64) 256         separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 192, 256, 64) 256         separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 192, 256, 64) 256         separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 192, 256, 2)  194         batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 192, 256, 2)  706         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 192, 256, 2)  194         batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 192, 256, 2)  194         batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 192, 256, 2)  194         batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 192, 256, 2)  8           separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 192, 256, 2)  8           separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 192, 256, 2)  8           separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 192, 256, 2)  8           separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 192, 256, 2)  8           separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          131328      global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 192, 256, 4)  0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192, 256, 4)  0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192, 256, 4)  0           batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 192, 256, 4)  0           batch_normalization_20[0][0]     \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tool (Conv2D)                   (None, 192, 256, 1)  5           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "midline (Conv2D)                (None, 192, 256, 1)  5           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tooltip (Conv2D)                (None, 192, 256, 1)  5           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "edgeline (Conv2D)               (None, 192, 256, 1)  5           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "detection (Dense)               (None, 2)            514         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,620,963\n",
      "Trainable params: 17,613,263\n",
      "Non-trainable params: 7,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahmoud/anaconda3/envs/torchgpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "133/133 [==============================] - 204s 364ms/step - loss: 2.8457 - tool_loss: 1.0890 - midline_loss: 0.0260 - tooltip_loss: 0.0242 - edgeline_loss: 0.0295 - detection_loss: 1.6771 - tool_IoU: 0.2645 - midline_mae: 0.0427 - tooltip_mae: 0.0396 - edgeline_mae: 0.0484 - detection_acc: 0.5189 - val_loss: 1.6218 - val_tool_loss: 0.8232 - val_midline_loss: 0.0333 - val_tooltip_loss: 0.0322 - val_edgeline_loss: 0.0403 - val_detection_loss: 0.6927 - val_tool_IoU: 0.3103 - val_midline_mae: 0.0726 - val_tooltip_mae: 0.0703 - val_edgeline_mae: 0.0852 - val_detection_acc: 0.5161\n",
      "Epoch 2/150\n",
      " 65/133 [=============>................] - ETA: 23s - loss: 1.4083 - tool_loss: 0.6737 - midline_loss: 0.0050 - tooltip_loss: 0.0025 - edgeline_loss: 0.0087 - detection_loss: 0.7184 - tool_IoU: 0.4414 - midline_mae: 0.0154 - tooltip_mae: 0.0115 - edgeline_mae: 0.0218 - detection_acc: 0.5305"
     ]
    }
   ],
   "source": [
    "CurrentDirectory=os.getcwd()\n",
    "\n",
    "\n",
    "datatrain = np.load('/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/DataFinal/training_data.npy',allow_pickle = True)\n",
    "x_train, x_valid = train_test_split(datatrain,test_size=0.15, shuffle= True)\n",
    "print(x_train.shape, x_valid.shape)\n",
    "\n",
    "TrainGen = DataGenerator(data= x_train, batch_size=5)\n",
    "TestGen = DataGenerator(data= x_valid, batch_size=5)\n",
    "\n",
    "\n",
    "#datatest = np.load('/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/DataFinal/testing_data.npy',allow_pickle = True)\n",
    "\n",
    "\n",
    "model = SIMOCNN(2, 192, 256)\n",
    "#modelSavePath = '/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/Models/ModelLR_1/FinalModel.hdf5'\n",
    "\n",
    "\n",
    "plot_model(model, show_shapes=True, to_file='Graph of ART-Net.png')\n",
    "model.summary()\n",
    "\n",
    "optim = Adadelta(learning_rate=1)\n",
    "\n",
    "model.compile(optimizer = optim, \n",
    "              loss = {'tool':bce_IoU_loss,\n",
    "                      'midline':'mean_squared_error',\n",
    "                      'tooltip':'mean_squared_error',\n",
    "                      'edgeline':'mean_squared_error',\n",
    "                      'detection':'categorical_crossentropy',}, \n",
    "              metrics = {'tool': IoU,\n",
    "                         'midline':'mae',\n",
    "                         'tooltip':'mae',\n",
    "                         'edgeline':'mae',\n",
    "                         'detection': 'acc'})\n",
    "\n",
    "checkpoint_path = '/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/Models/FullTraining/ModelLR1/Checkpoint'\n",
    "modelSavePath = '/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/Models/FullTraining/ModelLR1/fullmodel26-5-2021.h5'\n",
    "    \n",
    "model_checkpoint = [\n",
    "tf.keras.callbacks.ModelCheckpoint(checkpoint_path,save_best_only=True,save_weights_only=True, save_freq = 'epoch' ),\n",
    "tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=20, mode='min'), ## new_lr = lr * factor # 5\n",
    "tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=40, mode='min', restore_best_weights=True),    \n",
    "tf.keras.callbacks.CSVLogger('/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/Models/FullTraining/ModelLR1/training26-5-2021.csv'),\n",
    "tf.keras.callbacks.TensorBoard(log_dir='/home/mahmoud/Desktop/laparoscopic-Tools-Segmentation/Models/FullTraining/ModelLR1/logs',write_graph=True),\n",
    "tf.keras.callbacks.TerminateOnNaN()]\n",
    "        \n",
    "\n",
    "#model_checkpoint = ModelCheckpoint('FineTunedmodel.hdf5', \n",
    "#                                   monitor='val_loss', \n",
    "#                                   verbose=1, \n",
    "#                                   save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_yaml = model.to_yaml()\n",
    "with open('modelSaved.yaml', \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "\n",
    "history=model.fit_generator(TrainGen, \n",
    "                             steps_per_epoch=133, \n",
    "                             epochs=150,                  \n",
    "                             verbose=1, \n",
    "                             validation_data= TestGen , \n",
    "                             validation_steps=31,\n",
    "                             callbacks=[model_checkpoint])\n",
    "\n",
    "model.save(modelSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training & validation loss values\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss Total')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train_total','val_total'], loc='upper right')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['tool_loss'])\n",
    "plt.plot(history.history['val_tool_loss'])\n",
    "plt.title('Model loss for the Tool Segmentation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train_tool_loss', 'val_tool_loss'], loc='upper right')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['midline_loss'])\n",
    "plt.plot(history.history['val_midline_loss'])\n",
    "plt.title('Model loss for the Midline line prediction')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train_midline_loss','val_midline_loss'], loc='upper right')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['tooltip_loss'])\n",
    "plt.plot(history.history['val_tooltip_loss'])\n",
    "plt.title('Model loss for the tool tip point prediction')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train_tooltip_loss','val_tooltip_loss'], loc='upper right')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['edgeline_loss'])\n",
    "plt.plot(history.history['val_edgeline_loss'])\n",
    "plt.title('Model loss for the edge line prediction')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train_edgeLine_loss','val_edgeLine_loss'], loc='upper right')\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['detection_loss'])\n",
    "plt.plot(history.history['val_detection_loss'])\n",
    "plt.title('Model loss for the detection ')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train_detection_loss','val_detection_loss'], loc='upper right')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-carol",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('torchgpu': conda)",
   "language": "python",
   "name": "python38664bittorchgpuconda9a1ebec0d7dc40ab979cec8bdf0d06bf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
